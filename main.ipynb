{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f34c98ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T20:49:24.147196Z",
     "start_time": "2025-03-20T20:49:23.878513Z"
    }
   },
   "source": [
    "# **A Novel Explainable Approach for Intrusion Detection**\n",
    "\n",
    "This notebook presents the code for the the paper **A Novel Explainable Approach for Intrusion Detection**.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f0239e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd2af90",
   "metadata": {},
   "source": [
    "# Reading the dataset\n",
    "\n",
    "To access the **CIC-IDS 2017** dataset, follow these steps:\n",
    "\n",
    "1. Visit the official dataset page: [CIC-IDS 2017](https://www.unb.ca/cic/datasets/ids-2017.html).\n",
    "2. Scroll to the bottom of the page and click on **\"Download this dataset\"**.\n",
    "3. Fill in the required details, including your **name, email address, and other requested information**.\n",
    "4. Click on the **Submit** button.\n",
    "5. Once submitted, a page will appear containing a folder named **CIC-IDS-2017**.\n",
    "6. Navigate to the **CSVs** folder inside this directory.\n",
    "7. Download the file **MachineLearningCSV.zip** and extract its contents.\n",
    "\n",
    "After extraction, you will find the following CSV files:\n",
    "\n",
    "- Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv \n",
    "- Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv \n",
    "- Friday-WorkingHours-Morning.pcap_ISCX.csv      \n",
    "- Monday-WorkingHours.pcap_ISCX.csv            \n",
    "- Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv \n",
    "- Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv \n",
    "- Tuesday-WorkingHours.pcap_ISCX.csv           \n",
    "- Wednesday-workingHours.pcap_ISCX.csv          \n",
    "\n",
    "\n",
    "For this research, we selected the file **`Wednesday-workingHours.pcap_ISCX.csv`** which contains **692,703 records** each consists of **68 attributes (features)** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cd380cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T20:49:39.147797Z",
     "start_time": "2025-03-20T20:49:36.897384Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set Pandas to display all rows without truncation (useful for large datasets)\n",
    "pd.set_option('display.max_row', None)\n",
    "\n",
    "# Set Pandas to display all columns without truncation (ensures all features are visible)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Read the CSV file into a Pandas DataFrame (loading network traffic data for analysis)\n",
    "# The file 'Wednesday-workingHours.pcap_ISCX.csv' must be in the same directory as the \n",
    "# 'main.ipynb'. Otherwise, you need to provide the full file path.\n",
    "\n",
    "df = pd.read_csv('Wednesday-workingHours.pcap_ISCX.csv')  # df: the original dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710f7106",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "\n",
    "The preprocessing consists of the following steps:  \n",
    "\n",
    "### **1. Handling Missing and Out-of-Range Values**  \n",
    "- Samples with missing or out-of-range values are **identified and removed** instead of being imputed.  \n",
    "- Imputation is avoided to maintain data integrity, as the number of affected samples is negligible.  \n",
    "\n",
    "### **2. Removing Zero-Variance Features**  \n",
    "- Features with **zero variance** (i.e., identical values across all records) are eliminated.  \n",
    "- These features do not contribute to model learning and can slow down computations.  \n",
    "- **Examples of zero-variance features:**  \n",
    "  - `Bwd PSH Flags`, `Fwd URG Flags`, `Bwd URG Flags`, `CWE Flag Count`, etc.  \n",
    "\n",
    "### **3. Eliminating Duplicate Features**  \n",
    "- Identical features providing redundant information are detected and removed.  \n",
    "- Retaining only one feature per duplicate set reduces computational overhead and improves model interpretability. ðŸš€  \n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7ac2876",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T20:51:54.929047Z",
     "start_time": "2025-03-20T20:51:52.433989Z"
    }
   },
   "outputs": [],
   "source": [
    "# data Cleaning (df_cleaned)\n",
    "df_cleaned = df.dropna()  # Drop rows with NaN values\n",
    "df_cleaned = df_cleaned[~df_cleaned.isin([np.inf, -np.inf]).any(axis=1)] # Drop rows with out of range values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49c87f00",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T20:52:13.885050Z",
     "start_time": "2025-03-20T20:52:13.780540Z"
    }
   },
   "outputs": [],
   "source": [
    "# Seprating labels and data\n",
    "data = df_cleaned.iloc[:,:-1]   # data contains clean attributes\n",
    "labels = df_cleaned.iloc[:,-1]  # labels contain the corresponding labels in categorical format ('BENIGN','DoS slowloris',... )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5766bb2c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T20:52:17.461270Z",
     "start_time": "2025-03-20T20:52:17.377474Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features with zero variance:\n",
      " Index([' Bwd PSH Flags', ' Fwd URG Flags', ' Bwd URG Flags', ' CWE Flag Count',\n",
      "       'Fwd Avg Bytes/Bulk', ' Fwd Avg Packets/Bulk', ' Fwd Avg Bulk Rate',\n",
      "       ' Bwd Avg Bytes/Bulk', ' Bwd Avg Packets/Bulk', 'Bwd Avg Bulk Rate'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# identifying zero-variance features\n",
    "zero_variance_features = data.var() == 0\n",
    "zero_variance_index = zero_variance_features[zero_variance_features].index\n",
    "print(\"Features with zero variance:\\n\", zero_variance_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78946304",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T20:52:22.170538Z",
     "start_time": "2025-03-20T20:52:22.080430Z"
    }
   },
   "outputs": [],
   "source": [
    "# Droping the specified columns (i.e., those with zero variance)\n",
    "data_dropped_zero_var = data.drop(columns= zero_variance_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a9401d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T20:52:39.662378Z",
     "start_time": "2025-03-20T20:52:38.307291Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identical columns: [(' Total Fwd Packets', 'Subflow Fwd Packets'), (' Total Backward Packets', ' Subflow Bwd Packets'), ('Total Length of Fwd Packets', ' Subflow Fwd Bytes'), (' Fwd Packet Length Mean', ' Avg Fwd Segment Size'), ('Fwd PSH Flags', ' SYN Flag Count'), (' Fwd Header Length', ' Fwd Header Length.1'), (' SYN Flag Count', 'Fwd PSH Flags'), (' Avg Fwd Segment Size', ' Fwd Packet Length Mean'), (' Fwd Header Length.1', ' Fwd Header Length'), ('Subflow Fwd Packets', ' Total Fwd Packets'), (' Subflow Fwd Bytes', 'Total Length of Fwd Packets'), (' Subflow Bwd Packets', ' Total Backward Packets')]\n"
     ]
    }
   ],
   "source": [
    "# Finding identical columns\n",
    "columns = data_dropped_zero_var.columns\n",
    "identical_columns = []\n",
    "\n",
    "for col1 in data_dropped_zero_var.columns:\n",
    "    for col2 in data_dropped_zero_var.columns:\n",
    "        if col1 != col2 and data_dropped_zero_var[col1].equals(data_dropped_zero_var[col2]):\n",
    "            identical_columns.append((col1, col2))\n",
    "\n",
    "print(\"Identical columns:\", identical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c383dda",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T20:53:09.511565Z",
     "start_time": "2025-03-20T20:53:09.495933Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(' Total Fwd Packets', 'Subflow Fwd Packets'), (' Fwd Header Length', ' Fwd Header Length.1'), (' SYN Flag Count', 'Fwd PSH Flags'), (' Subflow Bwd Packets', ' Total Backward Packets'), (' Subflow Fwd Bytes', 'Total Length of Fwd Packets'), (' Avg Fwd Segment Size', ' Fwd Packet Length Mean')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Use a set to filter out tuples considering (X, Y) and (Y, X) as the same\n",
    "unique_pairs = set()\n",
    "\n",
    "# Add each tuple to the set in a sorted order (to avoid duplicates with reversed order)\n",
    "for pair in identical_columns:\n",
    "    unique_pairs.add(tuple(sorted(pair)))\n",
    "\n",
    "# Convert back to list to maintain the same structure as original input\n",
    "unique_tuples_list = list(unique_pairs)\n",
    "\n",
    "# Output the result\n",
    "print(unique_tuples_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8fbb844",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T20:53:12.614758Z",
     "start_time": "2025-03-20T20:53:12.583587Z"
    }
   },
   "outputs": [],
   "source": [
    "# Extract the columns to drop (second item of each tuple)\n",
    "columns_to_drop = [pair[1] for pair in unique_tuples_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85b43e20",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T20:53:16.902019Z",
     "start_time": "2025-03-20T20:53:16.805122Z"
    }
   },
   "outputs": [],
   "source": [
    "# Drop the duplicate columns from the DataFrame\n",
    "data_var_ident_dropped = data_dropped_zero_var.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a348c3",
   "metadata": {},
   "source": [
    "# Covariance Matrix of Preprocessed Data\n",
    "\n",
    "After completing the preprocessing steps, we calculate the covariance matrix of the remaining features. Before this calculation, feature scaling is applied by standardizing the features. The resulting covariance matrix has several key properties:  \n",
    "\n",
    "- **Diagonal Values**: All diagonal values are large, indicating that features with zero variance have been removed during preprocessing.  \n",
    "- **Symmetry**: The covariance matrix is symmetric, as expected.  \n",
    "- **Feature Covariance**: The matrix reveals strong covariance among certain features, suggesting a high degree of correlation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6a09715",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T20:53:22.473467Z",
     "start_time": "2025-03-20T20:53:21.611746Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(data_var_ident_dropped)  # it converts pandas dataframe into a numpy array\n",
    "##########################################################\n",
    "# After applying StandardScaler to data_var_ident_dropped, \n",
    "# the resulting scaled_data is a NumPy array without any\n",
    "# column names. To retrieve the corresponding column names,\n",
    "# you can use the column names from the original DataFrame,\n",
    "# 'data_var_ident_dropped'.\n",
    "##########################################################\n",
    "\n",
    "\n",
    "# Compute the Covariance Matrix (for the standardized data)\n",
    "cov_matrix = np.cov(scaled_data, rowvar=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f677b4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T20:53:32.123754Z",
     "start_time": "2025-03-20T20:53:30.995127Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "# Set the plot size\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Create the heatmap using seaborn\n",
    "sns.heatmap(cov_matrix, annot=False, cmap=\"coolwarm\", fmt='.2f', linewidths=0.5)\n",
    "\n",
    "# Add labels and title\n",
    "plt.title('Covariance Matrix Heatmap', fontsize=12)\n",
    "plt.xlabel('Features', fontsize=12)\n",
    "plt.ylabel('Features', fontsize=12)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2266bda",
   "metadata": {},
   "source": [
    "# Changing the labels\n",
    "\n",
    "In the dataset, the class *BENIGN* represents normal activities, while the other five classes correspond to different types of malicious attacks. To align with an anomaly detection framework, the labels are modified as follows:\n",
    "- **Label 0** is assigned to **BENIGN** samples (*normal activities*).\n",
    "- **Label 1** is assigned to all other samples (*malicious attacks*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11481d0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T20:53:38.999138Z",
     "start_time": "2025-03-20T20:53:38.877914Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert categorical labels to numerical values\n",
    "labels_numeric = labels.apply(lambda x: 0 if x == 'BENIGN' else 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf57e3f",
   "metadata": {},
   "source": [
    "# Combine Cleaned Data with Corresponding Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02ebb9c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T20:53:44.426314Z",
     "start_time": "2025-03-20T20:53:44.319513Z"
    }
   },
   "outputs": [],
   "source": [
    "# Concatenate labels with cleaned data\n",
    "cleaned_data_labels = pd.concat([data_var_ident_dropped, labels_numeric.rename('Label')], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cd2565",
   "metadata": {},
   "source": [
    "# Define Features and Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19044f57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T20:53:59.403047Z",
     "start_time": "2025-03-20T20:53:59.273363Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define features and target variable\n",
    "X = cleaned_data_labels.iloc[:, :-1]  # features: all columns except the last one \n",
    "y = cleaned_data_labels.iloc[:, -1]   # targets : last column "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d7edae",
   "metadata": {},
   "source": [
    "# Training XGBoost and extracting importance scores\n",
    "\n",
    "### Overview\n",
    "Understanding which features contribute most to model predictions is crucial for improving performance and interpretability. XGBoost provides multiple ways to measure feature importance, helping us identify the most influential features in the dataset. \n",
    "\n",
    "In this section, we:\n",
    "- Train an **XGBoost model** on preprocessed data.\n",
    "- Extract three different **feature importance** scores.\n",
    "- Normalize and visualize the importance scores.\n",
    "\n",
    "By analyzing feature importance, we can gain insights into which features are most useful for classification and potentially refine our feature selection process.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Splitting Data\n",
    "The dataset is divided into **training** and **validation** sets using `train_test_split`. This allows us to evaluate the modelâ€™s performance on unseen data.\n",
    "\n",
    "### 2. Training the XGBoost Model\n",
    "- The dataset is converted into **DMatrix**, an optimized data structure for XGBoost.\n",
    "- Model parameters are defined, including:\n",
    "  - `objective='binary:logistic'`: Suitable for binary classification.\n",
    "  - `max_depth=4`: Limits tree depth to control model complexity.\n",
    "  - `eta=0.1`: Learning rate to adjust step size in training.\n",
    "  - `eval_metric='logloss'`: Measures the performance using **logarithmic loss**.\n",
    "- The model is trained using `xgb.train()` with **early stopping** to prevent overfitting.\n",
    "\n",
    "### 3. Calculating Feature Importance\n",
    "XGBoost provides feature importance based on three metrics:\n",
    "- **Gain**: The average contribution of a feature when used for splitting (refer to **Section 3** of the paper for more details).\n",
    "- **Cover**: The number of samples affected when a feature is used for splitting.\n",
    "- **Weight**: The frequency a feature is used in boosting trees.\n",
    "\n",
    "### 4. Normalizing Importance Values\n",
    "- The calculated importance scores are stored in a **DataFrame**.\n",
    "- `MinMaxScaler` is used to **scale values between 0 and 1**, making comparisons easier.\n",
    "- The features are **sorted by normalized gain**, highlighting the most influential ones.\n",
    "\n",
    "### 5. Visualizing Feature Importance\n",
    "A **bar chart** is plotted to display feature importance scores (**gain, cover, and weight**).  \n",
    "- The x-axis represents **feature names**.\n",
    "- The y-axis shows **normalized importance scores**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e548fc1e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T20:54:07.566553Z",
     "start_time": "2025-03-20T20:54:01.423722Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.60287\teval-logloss:0.60300\n",
      "[1]\ttrain-logloss:0.52910\teval-logloss:0.52918\n",
      "[2]\ttrain-logloss:0.46758\teval-logloss:0.46775\n",
      "[3]\ttrain-logloss:0.41540\teval-logloss:0.41554\n",
      "[4]\ttrain-logloss:0.37097\teval-logloss:0.37109\n",
      "[5]\ttrain-logloss:0.33274\teval-logloss:0.33291\n",
      "[6]\ttrain-logloss:0.29958\teval-logloss:0.29973\n",
      "[7]\ttrain-logloss:0.26958\teval-logloss:0.26982\n",
      "[8]\ttrain-logloss:0.24318\teval-logloss:0.24348\n",
      "[9]\ttrain-logloss:0.22006\teval-logloss:0.22044\n",
      "[10]\ttrain-logloss:0.19988\teval-logloss:0.20027\n",
      "[11]\ttrain-logloss:0.18223\teval-logloss:0.18269\n",
      "[12]\ttrain-logloss:0.16591\teval-logloss:0.16638\n",
      "[13]\ttrain-logloss:0.15173\teval-logloss:0.15224\n",
      "[14]\ttrain-logloss:0.13905\teval-logloss:0.13960\n",
      "[15]\ttrain-logloss:0.12759\teval-logloss:0.12816\n",
      "[16]\ttrain-logloss:0.11730\teval-logloss:0.11788\n",
      "[17]\ttrain-logloss:0.10839\teval-logloss:0.10902\n",
      "[18]\ttrain-logloss:0.10046\teval-logloss:0.10113\n",
      "[19]\ttrain-logloss:0.09222\teval-logloss:0.09293\n",
      "[20]\ttrain-logloss:0.08469\teval-logloss:0.08543\n",
      "[21]\ttrain-logloss:0.07882\teval-logloss:0.07958\n",
      "[22]\ttrain-logloss:0.07274\teval-logloss:0.07347\n",
      "[23]\ttrain-logloss:0.06814\teval-logloss:0.06890\n",
      "[24]\ttrain-logloss:0.06319\teval-logloss:0.06394\n",
      "[25]\ttrain-logloss:0.05907\teval-logloss:0.05980\n",
      "[26]\ttrain-logloss:0.05504\teval-logloss:0.05577\n",
      "[27]\ttrain-logloss:0.05145\teval-logloss:0.05217\n",
      "[28]\ttrain-logloss:0.04838\teval-logloss:0.04909\n",
      "[29]\ttrain-logloss:0.04486\teval-logloss:0.04560\n",
      "[30]\ttrain-logloss:0.04169\teval-logloss:0.04244\n",
      "[31]\ttrain-logloss:0.03934\teval-logloss:0.04008\n",
      "[32]\ttrain-logloss:0.03671\teval-logloss:0.03748\n",
      "[33]\ttrain-logloss:0.03453\teval-logloss:0.03529\n",
      "[34]\ttrain-logloss:0.03259\teval-logloss:0.03334\n",
      "[35]\ttrain-logloss:0.03078\teval-logloss:0.03151\n",
      "[36]\ttrain-logloss:0.02918\teval-logloss:0.02989\n",
      "[37]\ttrain-logloss:0.02775\teval-logloss:0.02845\n",
      "[38]\ttrain-logloss:0.02647\teval-logloss:0.02715\n",
      "[39]\ttrain-logloss:0.02533\teval-logloss:0.02600\n",
      "[40]\ttrain-logloss:0.02429\teval-logloss:0.02496\n",
      "[41]\ttrain-logloss:0.02315\teval-logloss:0.02379\n",
      "[42]\ttrain-logloss:0.02214\teval-logloss:0.02277\n",
      "[43]\ttrain-logloss:0.02105\teval-logloss:0.02167\n",
      "[44]\ttrain-logloss:0.02002\teval-logloss:0.02058\n",
      "[45]\ttrain-logloss:0.01918\teval-logloss:0.01974\n",
      "[46]\ttrain-logloss:0.01832\teval-logloss:0.01888\n",
      "[47]\ttrain-logloss:0.01755\teval-logloss:0.01808\n",
      "[48]\ttrain-logloss:0.01681\teval-logloss:0.01734\n",
      "[49]\ttrain-logloss:0.01629\teval-logloss:0.01680\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and validation sets.\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Convert datasets into DMatrix, which is optimized for XGBoost.\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dvalid = xgb.DMatrix(X_valid, label=y_valid)\n",
    "#--------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Define model parameters.\n",
    "params = {\n",
    "    'objective': 'binary:logistic',      # Binary classification objective\n",
    "    'max_depth': 4,                      # Maximum depth of boosting trees\n",
    "    'eta': 0.1,                          # Learning rate (step size)\n",
    "    'eval_metric': 'logloss'             # Loss function to minimize (logarithmic loss)\n",
    "}\n",
    "#--------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Define watchlist.\n",
    "watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "\n",
    "\n",
    "num_rounds = 50         # Number of boosting rounds.\n",
    "evals_result = {}       # Dictionary to store evaluation results during training\n",
    "\n",
    "# Train the model\n",
    "model = xgb.train(params, dtrain, num_rounds, watchlist, early_stopping_rounds=10, evals_result=evals_result, verbose_eval=True)\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "# Define the types of feature importance to calculate: \n",
    "# 'gain' represents the average gain (improvement) brought by a feature in the model,\n",
    "# 'cover' represents the relative frequency a feature is used to split the data,\n",
    "# 'weight' represents the number of times a feature is used in trees.\n",
    "\n",
    "# Generate importance scores for each feature using the above types\n",
    "importance_types = ['gain', 'cover', 'weight']\n",
    "importance_values = {t: model.get_score(importance_type=t) for t in importance_types}\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Convert to DataFrame.\n",
    "importance_df = pd.DataFrame(importance_values).fillna(0)\n",
    "\n",
    "# Normalize each column to [0,1] range using MinMaxScaler.\n",
    "scaler = MinMaxScaler()\n",
    "importance_df[:] = scaler.fit_transform(importance_df)\n",
    "\n",
    "# Sort by normalized gain for better visualization\n",
    "importance_df = importance_df.sort_values(by='gain', ascending=False)\n",
    "#--------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Plot Feature Importance \n",
    "importance_df.plot(kind='bar', figsize=(12, 6), width=0.8, colormap='viridis')\n",
    "#plt.xlabel('Features',fontsize=14)\n",
    "plt.ylabel('Normalized Importance Score',fontsize=12)\n",
    "plt.title('Feature Importance (Normalized Gain, Cover, Weight)')\n",
    "plt.legend(title='Metric')\n",
    "plt.xticks(rotation=90)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Plot Gain Scores\n",
    "plt.figure(figsize=(12, 6))\n",
    "importance_df['gain'].plot(kind='bar', color='royalblue', width=0.8)\n",
    "\n",
    "#plt.xlabel('Features', fontsize=14)\n",
    "plt.ylabel('Normalized Weight Score', fontsize=12)\n",
    "# plt.title('Feature Importance (Gain Only)', fontsize=14)\n",
    "plt.xticks(rotation=90)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b889db02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gain</th>\n",
       "      <th>cover</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Bwd Packet Length Std</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.619524</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Packet Length Mean</th>\n",
       "      <td>0.874347</td>\n",
       "      <td>0.861828</td>\n",
       "      <td>0.191304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bwd Packets/s</th>\n",
       "      <td>0.186786</td>\n",
       "      <td>0.204659</td>\n",
       "      <td>0.191304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Destination Port</th>\n",
       "      <td>0.099629</td>\n",
       "      <td>0.127199</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Flow IAT Mean</th>\n",
       "      <td>0.092572</td>\n",
       "      <td>0.133998</td>\n",
       "      <td>0.078261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>URG Flag Count</th>\n",
       "      <td>0.067601</td>\n",
       "      <td>0.112332</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Active Std</th>\n",
       "      <td>0.061287</td>\n",
       "      <td>0.886808</td>\n",
       "      <td>0.086957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Init_Win_bytes_backward</th>\n",
       "      <td>0.060260</td>\n",
       "      <td>0.212840</td>\n",
       "      <td>0.504348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fwd Header Length</th>\n",
       "      <td>0.054233</td>\n",
       "      <td>0.115591</td>\n",
       "      <td>0.078261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bwd IAT Std</th>\n",
       "      <td>0.054226</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.043478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Min Packet Length</th>\n",
       "      <td>0.049132</td>\n",
       "      <td>0.117377</td>\n",
       "      <td>0.086957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Init_Win_bytes_forward</th>\n",
       "      <td>0.037507</td>\n",
       "      <td>0.050088</td>\n",
       "      <td>0.191304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total Length of Bwd Packets</th>\n",
       "      <td>0.035654</td>\n",
       "      <td>0.053707</td>\n",
       "      <td>0.095652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Flow Packets/s</th>\n",
       "      <td>0.034884</td>\n",
       "      <td>0.296304</td>\n",
       "      <td>0.104348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fwd IAT Min</th>\n",
       "      <td>0.030654</td>\n",
       "      <td>0.048446</td>\n",
       "      <td>0.086957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fwd Packet Length Std</th>\n",
       "      <td>0.029542</td>\n",
       "      <td>0.050638</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Idle Max</th>\n",
       "      <td>0.028839</td>\n",
       "      <td>0.534335</td>\n",
       "      <td>0.034783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Idle Mean</th>\n",
       "      <td>0.027099</td>\n",
       "      <td>0.083807</td>\n",
       "      <td>0.043478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Flow IAT Min</th>\n",
       "      <td>0.021835</td>\n",
       "      <td>0.068241</td>\n",
       "      <td>0.173913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fwd Packet Length Min</th>\n",
       "      <td>0.021177</td>\n",
       "      <td>0.340635</td>\n",
       "      <td>0.147826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average Packet Size</th>\n",
       "      <td>0.020813</td>\n",
       "      <td>0.038053</td>\n",
       "      <td>0.095652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bwd IAT Mean</th>\n",
       "      <td>0.020098</td>\n",
       "      <td>0.005964</td>\n",
       "      <td>0.026087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fwd Packet Length Max</th>\n",
       "      <td>0.018370</td>\n",
       "      <td>0.325624</td>\n",
       "      <td>0.034783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FIN Flag Count</th>\n",
       "      <td>0.015674</td>\n",
       "      <td>0.082042</td>\n",
       "      <td>0.165217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Subflow Bwd Packets</th>\n",
       "      <td>0.015645</td>\n",
       "      <td>0.015159</td>\n",
       "      <td>0.052174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bwd Packet Length Max</th>\n",
       "      <td>0.010624</td>\n",
       "      <td>0.011791</td>\n",
       "      <td>0.052174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Down/Up Ratio</th>\n",
       "      <td>0.009128</td>\n",
       "      <td>0.062309</td>\n",
       "      <td>0.017391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fwd Packets/s</th>\n",
       "      <td>0.008697</td>\n",
       "      <td>0.055637</td>\n",
       "      <td>0.026087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fwd IAT Mean</th>\n",
       "      <td>0.008665</td>\n",
       "      <td>0.001666</td>\n",
       "      <td>0.017391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bwd Packet Length Mean</th>\n",
       "      <td>0.008346</td>\n",
       "      <td>0.030211</td>\n",
       "      <td>0.286957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Flow Duration</th>\n",
       "      <td>0.007682</td>\n",
       "      <td>0.021459</td>\n",
       "      <td>0.086957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Max Packet Length</th>\n",
       "      <td>0.007140</td>\n",
       "      <td>0.021211</td>\n",
       "      <td>0.060870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ACK Flag Count</th>\n",
       "      <td>0.003449</td>\n",
       "      <td>0.003626</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Packet Length Std</th>\n",
       "      <td>0.002499</td>\n",
       "      <td>0.002799</td>\n",
       "      <td>0.026087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Active Mean</th>\n",
       "      <td>0.001958</td>\n",
       "      <td>0.022934</td>\n",
       "      <td>0.008696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min_seg_size_forward</th>\n",
       "      <td>0.001364</td>\n",
       "      <td>0.000850</td>\n",
       "      <td>0.008696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bwd Header Length</th>\n",
       "      <td>0.001164</td>\n",
       "      <td>0.016177</td>\n",
       "      <td>0.052174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Avg Fwd Segment Size</th>\n",
       "      <td>0.000962</td>\n",
       "      <td>0.020780</td>\n",
       "      <td>0.034783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Flow Bytes/s</th>\n",
       "      <td>0.000772</td>\n",
       "      <td>0.079893</td>\n",
       "      <td>0.017391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Subflow Fwd Bytes</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017391</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  gain     cover    weight\n",
       " Bwd Packet Length Std        1.000000  0.619524  0.400000\n",
       " Packet Length Mean           0.874347  0.861828  0.191304\n",
       " Bwd Packets/s                0.186786  0.204659  0.191304\n",
       " Destination Port             0.099629  0.127199  1.000000\n",
       " Flow IAT Mean                0.092572  0.133998  0.078261\n",
       " URG Flag Count               0.067601  0.112332  0.000000\n",
       " Active Std                   0.061287  0.886808  0.086957\n",
       " Init_Win_bytes_backward      0.060260  0.212840  0.504348\n",
       " Fwd Header Length            0.054233  0.115591  0.078261\n",
       " Bwd IAT Std                  0.054226  1.000000  0.043478\n",
       " Min Packet Length            0.049132  0.117377  0.086957\n",
       "Init_Win_bytes_forward        0.037507  0.050088  0.191304\n",
       " Total Length of Bwd Packets  0.035654  0.053707  0.095652\n",
       " Flow Packets/s               0.034884  0.296304  0.104348\n",
       " Fwd IAT Min                  0.030654  0.048446  0.086957\n",
       " Fwd Packet Length Std        0.029542  0.050638  0.000000\n",
       " Idle Max                     0.028839  0.534335  0.034783\n",
       "Idle Mean                     0.027099  0.083807  0.043478\n",
       " Flow IAT Min                 0.021835  0.068241  0.173913\n",
       " Fwd Packet Length Min        0.021177  0.340635  0.147826\n",
       " Average Packet Size          0.020813  0.038053  0.095652\n",
       " Bwd IAT Mean                 0.020098  0.005964  0.026087\n",
       " Fwd Packet Length Max        0.018370  0.325624  0.034783\n",
       "FIN Flag Count                0.015674  0.082042  0.165217\n",
       " Subflow Bwd Packets          0.015645  0.015159  0.052174\n",
       "Bwd Packet Length Max         0.010624  0.011791  0.052174\n",
       " Down/Up Ratio                0.009128  0.062309  0.017391\n",
       "Fwd Packets/s                 0.008697  0.055637  0.026087\n",
       " Fwd IAT Mean                 0.008665  0.001666  0.017391\n",
       " Bwd Packet Length Mean       0.008346  0.030211  0.286957\n",
       " Flow Duration                0.007682  0.021459  0.086957\n",
       " Max Packet Length            0.007140  0.021211  0.060870\n",
       " ACK Flag Count               0.003449  0.003626  0.000000\n",
       " Packet Length Std            0.002499  0.002799  0.026087\n",
       "Active Mean                   0.001958  0.022934  0.008696\n",
       " min_seg_size_forward         0.001364  0.000850  0.008696\n",
       " Bwd Header Length            0.001164  0.016177  0.052174\n",
       " Avg Fwd Segment Size         0.000962  0.020780  0.034783\n",
       "Flow Bytes/s                  0.000772  0.079893  0.017391\n",
       " Subflow Fwd Bytes            0.000000  0.000000  0.017391"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalized Importance Scores for most important features \n",
    "importance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a525978a",
   "metadata": {},
   "source": [
    "# Calculating the performance metrics for XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca75ea6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9960\n",
      "Precision: 0.9921\n",
      "Recall: 0.9970\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_pred_proba = model.predict(dvalid)  # Get probability predictions\n",
    "y_pred = (y_pred_proba >= 0.5).astype(int)  # Convert probabilities to binary labels (0 or 1)\n",
    "\n",
    "# Compute performance metrics\n",
    "accuracy = accuracy_score(y_valid, y_pred)\n",
    "precision = precision_score(y_valid, y_pred)\n",
    "recall = recall_score(y_valid, y_pred)\n",
    "\n",
    "# Print the results\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f72180f",
   "metadata": {},
   "source": [
    "# Compression to Decision Trees\n",
    "\n",
    "To improve interpretability while maintaining performance, the proposed method compresses an **XGBoost model** into a **simplified decision tree**. \n",
    "\n",
    "### Key Steps:\n",
    "- **Feature Selection**: Gain scores are used to select **the most discriminative features**.\n",
    "- **Model Transformation**: Instead of using the complex ensemble of boosting trees, a decision tree is trained on the selected features.\n",
    "- **Improved Interpretability**: This transformation makes predictions easier to understand without significantly compromising accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d83d1630",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_list = importance_df.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2d1c227e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample the dataset randomly\n",
    "N = 40000\n",
    "SampledDataset = cleaned_data_labels.sample(n=N,random_state = 45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e4e34832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target variable\n",
    "X = SampledDataset.iloc[:, :-1]  # All columns except the last one as features\n",
    "y = SampledDataset.iloc[:, -1]   # Last column as the target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498f0c05",
   "metadata": {},
   "source": [
    "## Feature Selection and Decision Tree Training\n",
    "\n",
    "This section evaluates how model performance varies as different numbers of important features are used.\n",
    "\n",
    "### Steps:\n",
    "1. **Data Splitting**:  \n",
    "   - The dataset is divided into **training (80%)** and **testing (20%)** sets.\n",
    "\n",
    "2. **Iterative Feature Selection & Model Training**:  \n",
    "   - A loop iterates through different number of feature.\n",
    "   - The **Decision Tree Classifier** (with `max_depth=depth`) is trained on each subset of reduced features and tested on the corresponding test set.\n",
    "   - `depth` could be defined by the user (3, 4, and 5 were considered in the paper).\n",
    "\n",
    "3. **Model Evaluation**:  \n",
    "   - After training, the model predicts labels for the test set.\n",
    "   - **Precision, recall, and accuracy** are calculated for each iteration.\n",
    "\n",
    "4. **Results Storage & Model Saving**:  \n",
    "   - Performance metrics are stored in lists.\n",
    "   - Each trained model is saved using `joblib.dump()` for future use.\n",
    "\n",
    "5. **DataFrame for Comparison**:  \n",
    "   - The results are converted into a DataFrame, making it easier to compare model performance across different numbers of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ddbea7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.tree import DecisionTreeClassifier   # Decision Tree model\n",
    "import joblib                                     # For saving the trained models\n",
    "\n",
    "num_of_important_features = len(index_list)       # Get the number of important features from the provided index list\n",
    "\n",
    "# Split the dataset into training and testing sets (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize lists to store evaluation metrics and results\n",
    "results = []\n",
    "precision_list = []\n",
    "recall_list = []\n",
    "accuracy_list = []\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "# Iterate through different numbers of important features\n",
    "for feature_num in range(1,num_of_important_features):\n",
    "    # Select a subset of features to include in training and testing\n",
    "    columns_to_keep = index_list[0:feature_num]\n",
    "    X_train_subset = X_train.loc[:,columns_to_keep]\n",
    "    X_test_subset = X_test.loc[:,columns_to_keep]\n",
    "    \n",
    "    # Initialize and train a Decision Tree classifier with a max depth of 4\n",
    "    model = DecisionTreeClassifier(random_state=42,max_depth=4)\n",
    "    model.fit(X_train_subset, y_train)\n",
    "    \n",
    "    # # Make predictions on the test set\n",
    "    y_pred = model.predict(X_test_subset)\n",
    "    \n",
    "    # Calculate evaluation metrics: precision, recall, and accuracy\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Store computed metrics in respective lists\n",
    "    precision_list.append(precision)\n",
    "    recall_list.append(recall)\n",
    "    accuracy_list.append(accuracy)\n",
    "    \n",
    "    \n",
    "   # Store the results for this feature subset\n",
    "    results.append((feature_num, precision, recall, accuracy))\n",
    "   # Save the trained model for this feature subset\n",
    "    joblib.dump(model, f\"decision_tree_{feature_num}.joblib\")\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "# Convert results list into a DataFrame for better readability and analysis\n",
    "results_df = pd.DataFrame(results, columns=['Number of Features', 'Precision', 'Recall', 'Accuracy'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1ae850d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number of Features</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.996230</td>\n",
       "      <td>0.638812</td>\n",
       "      <td>0.868375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.850935</td>\n",
       "      <td>0.973757</td>\n",
       "      <td>0.928750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.945228</td>\n",
       "      <td>0.971340</td>\n",
       "      <td>0.969250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.990886</td>\n",
       "      <td>0.938536</td>\n",
       "      <td>0.974625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.981488</td>\n",
       "      <td>0.952003</td>\n",
       "      <td>0.976125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.981488</td>\n",
       "      <td>0.952003</td>\n",
       "      <td>0.976125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.991292</td>\n",
       "      <td>0.943370</td>\n",
       "      <td>0.976500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.984380</td>\n",
       "      <td>0.979282</td>\n",
       "      <td>0.986875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.984380</td>\n",
       "      <td>0.979282</td>\n",
       "      <td>0.986875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.984440</td>\n",
       "      <td>0.983080</td>\n",
       "      <td>0.988250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>0.984440</td>\n",
       "      <td>0.983080</td>\n",
       "      <td>0.988250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>0.984440</td>\n",
       "      <td>0.983080</td>\n",
       "      <td>0.988250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>0.984440</td>\n",
       "      <td>0.983080</td>\n",
       "      <td>0.988250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>0.984440</td>\n",
       "      <td>0.983080</td>\n",
       "      <td>0.988250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>0.984440</td>\n",
       "      <td>0.983080</td>\n",
       "      <td>0.988250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>0.984440</td>\n",
       "      <td>0.983080</td>\n",
       "      <td>0.988250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>0.984440</td>\n",
       "      <td>0.983080</td>\n",
       "      <td>0.988250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>0.984440</td>\n",
       "      <td>0.983080</td>\n",
       "      <td>0.988250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>0.984440</td>\n",
       "      <td>0.983080</td>\n",
       "      <td>0.988250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>0.984440</td>\n",
       "      <td>0.983080</td>\n",
       "      <td>0.988250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>0.984440</td>\n",
       "      <td>0.983080</td>\n",
       "      <td>0.988250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>0.984440</td>\n",
       "      <td>0.983080</td>\n",
       "      <td>0.988250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>0.984440</td>\n",
       "      <td>0.983080</td>\n",
       "      <td>0.988250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>0.984440</td>\n",
       "      <td>0.983080</td>\n",
       "      <td>0.988250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>0.984440</td>\n",
       "      <td>0.983080</td>\n",
       "      <td>0.988250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>0.984440</td>\n",
       "      <td>0.983080</td>\n",
       "      <td>0.988250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>0.984440</td>\n",
       "      <td>0.983080</td>\n",
       "      <td>0.988250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>0.984440</td>\n",
       "      <td>0.983080</td>\n",
       "      <td>0.988250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>0.984440</td>\n",
       "      <td>0.983080</td>\n",
       "      <td>0.988250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>0.984440</td>\n",
       "      <td>0.983080</td>\n",
       "      <td>0.988250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>31</td>\n",
       "      <td>0.984440</td>\n",
       "      <td>0.983080</td>\n",
       "      <td>0.988250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32</td>\n",
       "      <td>0.984440</td>\n",
       "      <td>0.983080</td>\n",
       "      <td>0.988250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>33</td>\n",
       "      <td>0.984440</td>\n",
       "      <td>0.983080</td>\n",
       "      <td>0.988250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>34</td>\n",
       "      <td>0.984440</td>\n",
       "      <td>0.983080</td>\n",
       "      <td>0.988250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>35</td>\n",
       "      <td>0.984440</td>\n",
       "      <td>0.983080</td>\n",
       "      <td>0.988250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>36</td>\n",
       "      <td>0.984440</td>\n",
       "      <td>0.983080</td>\n",
       "      <td>0.988250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>37</td>\n",
       "      <td>0.984440</td>\n",
       "      <td>0.983080</td>\n",
       "      <td>0.988250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>38</td>\n",
       "      <td>0.984440</td>\n",
       "      <td>0.983080</td>\n",
       "      <td>0.988250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>39</td>\n",
       "      <td>0.984440</td>\n",
       "      <td>0.983080</td>\n",
       "      <td>0.988250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Number of Features  Precision    Recall  Accuracy\n",
       "0                    1   0.996230  0.638812  0.868375\n",
       "1                    2   0.850935  0.973757  0.928750\n",
       "2                    3   0.945228  0.971340  0.969250\n",
       "3                    4   0.990886  0.938536  0.974625\n",
       "4                    5   0.981488  0.952003  0.976125\n",
       "5                    6   0.981488  0.952003  0.976125\n",
       "6                    7   0.991292  0.943370  0.976500\n",
       "7                    8   0.984380  0.979282  0.986875\n",
       "8                    9   0.984380  0.979282  0.986875\n",
       "9                   10   0.984440  0.983080  0.988250\n",
       "10                  11   0.984440  0.983080  0.988250\n",
       "11                  12   0.984440  0.983080  0.988250\n",
       "12                  13   0.984440  0.983080  0.988250\n",
       "13                  14   0.984440  0.983080  0.988250\n",
       "14                  15   0.984440  0.983080  0.988250\n",
       "15                  16   0.984440  0.983080  0.988250\n",
       "16                  17   0.984440  0.983080  0.988250\n",
       "17                  18   0.984440  0.983080  0.988250\n",
       "18                  19   0.984440  0.983080  0.988250\n",
       "19                  20   0.984440  0.983080  0.988250\n",
       "20                  21   0.984440  0.983080  0.988250\n",
       "21                  22   0.984440  0.983080  0.988250\n",
       "22                  23   0.984440  0.983080  0.988250\n",
       "23                  24   0.984440  0.983080  0.988250\n",
       "24                  25   0.984440  0.983080  0.988250\n",
       "25                  26   0.984440  0.983080  0.988250\n",
       "26                  27   0.984440  0.983080  0.988250\n",
       "27                  28   0.984440  0.983080  0.988250\n",
       "28                  29   0.984440  0.983080  0.988250\n",
       "29                  30   0.984440  0.983080  0.988250\n",
       "30                  31   0.984440  0.983080  0.988250\n",
       "31                  32   0.984440  0.983080  0.988250\n",
       "32                  33   0.984440  0.983080  0.988250\n",
       "33                  34   0.984440  0.983080  0.988250\n",
       "34                  35   0.984440  0.983080  0.988250\n",
       "35                  36   0.984440  0.983080  0.988250\n",
       "36                  37   0.984440  0.983080  0.988250\n",
       "37                  38   0.984440  0.983080  0.988250\n",
       "38                  39   0.984440  0.983080  0.988250"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "64d55cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance metric curves\n",
    "plt.plot(range(1, len(precision_list) + 1), precision_list, marker='o', label='Precision')\n",
    "plt.plot(range(1, len(recall_list) + 1), recall_list, marker='s', label='Recall')\n",
    "plt.plot(range(1, len(accuracy_list) + 1), accuracy_list, marker='^', label='Accuracy')\n",
    "\n",
    "plt.xlabel('Number of features')\n",
    "plt.ylabel('Scores')\n",
    "plt.title('Model performance metrics vs. number of features')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8963cf3",
   "metadata": {},
   "source": [
    "## Decision Tree Visualization\n",
    "\n",
    "### Overview\n",
    "To better understand the structure of the trained **Decision Tree Classifier**, we visualize it using **`graphviz`**.\n",
    "\n",
    "### Steps:\n",
    "1. **Select a Model**:  \n",
    "   - Choose a trained decision tree based on the number of features used.\n",
    "   - The model is loaded using `joblib.load()`.\n",
    "\n",
    "2. **Retrieve Feature Names**:  \n",
    "   - The corresponding feature names are extracted from `index_list` for proper labeling in the visualization.\n",
    "\n",
    "3. **Export the Tree**:  \n",
    "   - The `export_graphviz()` function converts the tree structure into a **DOT format** string.\n",
    "   - The visualization includes:\n",
    "     - Feature names at each decision node.\n",
    "     - Class labels (`Class 0` for BENIGN and `Class 1` for anomalies).\n",
    "     \n",
    "\n",
    "4. **Render and Display**:  \n",
    "   - The decision tree is rendered using `graphviz.Source()`.\n",
    "   - The tree is **saved** as a file (e.g., **PDF** format).\n",
    "   - The `graph.view()` command opens the saved visualization.\n",
    "\n",
    "### Purpose:\n",
    "This visualization helps in **interpreting the decision-making process** of the model. By examining feature splits and their thresholds, we can gain insights into how the model differentiates between classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9f2f7376",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'decision_tree_8.pdf'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries for visualizing the decision tree\n",
    "from sklearn.tree import export_graphviz\n",
    "import graphviz\n",
    "import joblib\n",
    "\n",
    "# Select the number of features to visualize the relevent decision tree\n",
    "feature_num = 8  # Change this value to visualize a model trained with a different number of features\n",
    "\n",
    "# Load the trained Decision Tree model corresponding to the selected feature subset\n",
    "model = joblib.load(f\"decision_tree_{feature_num}.joblib\")\n",
    "\n",
    "# Get the corresponding feature names\n",
    "columns_to_keep = index_list[:feature_num]\n",
    "\n",
    "# Export tree as a DOT format string\n",
    "dot_data = export_graphviz(model, out_file=None, \n",
    "                           feature_names=columns_to_keep,  \n",
    "                           class_names=['Class 0', 'Class 1'],  \n",
    "                           filled=True, rounded=True, special_characters=True)  \n",
    "\n",
    "# Render and display the tree\n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph.render(f\"decision_tree_{feature_num}\")  # Saves as 'decision_tree_7.pdf' or other formats\n",
    "graph.view()  # Opens the saved file\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
